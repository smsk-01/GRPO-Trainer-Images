# ğŸ–¼ï¸ GRPOÂ ImagesÂ Trainer

Fineâ€‘tune **QwenÂ 2.5â€‘VL** (or any Visionâ€‘Language model with the same API) on image *grounding* tasks using **GRPOÂ (GenericÂ RewardÂ PredictionÂ Optimization)** in just a few lines of code.

<p align="center">
  <img src="https://img.shields.io/badge/python-3.8%2B-blue?logo=python" />
  <img src="https://img.shields.io/badge/License-MIT-green.svg" />
</p>

---

## âœ¨Â Why use this repo?

* **Plugâ€‘andâ€‘play trainer** â€“ drop in your own JSON dataset of promptsÂ +Â boundingâ€‘boxes and start training.  
* **Imageâ€‘aware data collator** â€“ automatically loads, preprocesses and batches images.  
* **Rewardâ€‘based optimisation** â€“ leverages the `trl` libraryâ€™s GRPO algorithm for RLâ€‘style fineâ€‘tuning.  
* **Minimal codebase** â€“ only three Python files, easy to read and customise.  


## ğŸ”Â Under the hood


### `PersonalizedGRPOTrainer`Â (extendsÂ `trl.GRPOTrainer`)
* Accepts an `image_processor` and an `images_root` folder.  
* Overrides **`data_collator`** to  
  1. Load images with **Pillow**.  
  2. Batchâ€‘encode them via the HuggingÂ Face `AutoProcessor`.  
  3. Return a dict containing  
     * `pixel_values`Â â€“ tensor *(CÂ Ã—Â HÂ Ã—Â W)*  
     * `prompt`Â â€“ instruction string  
     * `solution`Â â€“ groundâ€‘truth bbox or coordinates  
     * `scales`Â â€“ original image size  

### `Qwen2_5_VLForConditionalGenerationWithLogits`Â (wrapper)
Tiny subclass that forwards all arguments to the real **QwenÂ 2.5â€‘VL** model while gracefully ignoring the extra `logits_to_keep` parameter expected by GRPO.

### Reward functionsÂ (`rewards.py`)
Currently only **`accuracy_reward_coord`**, which returns **1** if the (x,Â y) coordinate predicted by the model falls inside the groundâ€‘truth boundingâ€‘box and **0** otherwise.  
Feel free to add IoUâ€‘ or distanceâ€‘based rewards here.

### Training scriptÂ (`main.py`)
Provides a concrete example wiring everything together.  
Customise the constants at the top, or replace them with **argparse** flags for production use.

---

## âš™ï¸Â Configuration tips

| Hyperâ€‘parameter                   | Where to set   | Notes                                                     |
|----------------------------------|----------------|-----------------------------------------------------------|
| `per_device_train_batch_size`    | `GRPOConfig`   | Limited by GPU memory â€“ images are heavy!                |
| `num_generations`                | `GRPOConfig`   | How many action samples to draw per prompt.              |
| `reward_funcs`                   | trainer init   | List of callables returning a rewardÂ âˆˆÂ {0,Â 1}.           |
| `bf16` / `fp16`                  | `GRPOConfig`   | Use `bf16` on A100/H100 for speed and memory efficiency. |

---


